
<header>
<h1 class="p-name">A new  efficient lightweight semantic segmentation Network(E-UNet)</h1>
</header>
<section data-field="subtitle" class="p-summary">

</section>
<section data-field="body" class="e-content">
<section name="2c33" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="16d5" id="16d5" class="graf graf--h3 graf--leading graf--title">A new efficient lightweight semantic segmentation Network(E-UNet)</h3><figure name="b088" id="b088" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*8-EbD0OkVEWo7AySaK50Hw.png" data-width="1992" data-height="1112" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*8-EbD0OkVEWo7AySaK50Hw.png"></figure><h3 name="d278" id="d278" class="graf graf--h3 graf-after--figure">1. Introduction</h3><p name="8541" id="8541" class="graf graf--p graf-after--h3">As explained in <a href="https://medium.com/analytics-vidhya/deep-learning-semantic-segmentation-networks-18148e2cf0fb" data-href="https://medium.com/analytics-vidhya/deep-learning-semantic-segmentation-networks-18148e2cf0fb" class="markup--anchor markup--p-anchor" target="_blank">the previous post</a>, <strong class="markup--strong markup--p-strong">semantic segmentation</strong> can be described as <strong class="markup--strong markup--p-strong">classifying each pixel into a specific class</strong> which can be employed for many goals such as satellite imagery analysis to on-the-fly visual search, preservation of the cultural heritage up to the recognition of image copies and human-computer interaction.</p><blockquote name="f049" id="f049" class="graf graf--blockquote graf-after--p"><strong class="markup--strong markup--blockquote-strong"><em class="markup--em markup--blockquote-em">Different deep learning models have been introduced to segment objects semantically, but the ability and need to perform pixel-by-pixel semantic segmentation in real time and with fewer FLOPs (floating point operations per second) to achieve similar or better accuracy than existing models such as U-net is one of the major challenges that must be considered.</em></strong></blockquote><p name="73d8" id="73d8" class="graf graf--p graf-after--blockquote">To overcome this challenge I tried to improve the <strong class="markup--strong markup--p-strong">ENet (</strong><a href="https://arxiv.org/abs/1606.02147" data-href="https://arxiv.org/abs/1606.02147" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">A Deep Neural Network Architecture for Real-Time Semantic Segmentation</strong></a><strong class="markup--strong markup--p-strong">) </strong>by removing some layers and adding skip connection to have<strong class="markup--strong markup--p-strong"> a lightweight semantic segmentation network called</strong> <strong class="markup--strong markup--p-strong">E-UNet</strong> which looks like a “U”.</p><h3 name="dd67" id="dd67" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">2. Model</strong></h3><p name="cd7a" id="cd7a" class="graf graf--p graf-after--h3">The architecture of the <strong class="markup--strong markup--p-strong">E-UNet </strong>consists of three sections, including:</p><blockquote name="8889" id="8889" class="graf graf--blockquote graf-after--p"><strong class="markup--strong markup--blockquote-strong"><em class="markup--em markup--blockquote-em">1.The contraction section</em></strong></blockquote><blockquote name="f8d3" id="f8d3" class="graf graf--blockquote graf-after--blockquote"><strong class="markup--strong markup--blockquote-strong"><em class="markup--em markup--blockquote-em">2.The expansion section</em></strong></blockquote><blockquote name="2519" id="2519" class="graf graf--blockquote graf-after--blockquote"><strong class="markup--strong markup--blockquote-strong"><em class="markup--em markup--blockquote-em">3. The skip connection</em></strong></blockquote><h4 name="1d00" id="1d00" class="graf graf--h4 graf-after--blockquote"><strong class="markup--strong markup--h4-strong">2.1. The contraction section:</strong></h4><p name="bf58" id="bf58" class="graf graf--p graf-after--h4">This section is used to capture the context of object of interest ( without concerning the location of object) in images and consists of :</p><p name="4bd1" id="4bd1" class="graf graf--p graf-after--p">(a) <strong class="markup--strong markup--p-strong">E-UNet</strong> initial block. MaxPooling is performed with non-overlapping 2×2 windows, and the convolution has 13 filters that sum to 16 feature maps after concatenation.</p><figure name="c244" id="c244" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*8TuBU8B2zHK2Mv9a5yhxUw.png" data-width="1354" data-height="542" src="https://cdn-images-1.medium.com/max/800/1*8TuBU8B2zHK2Mv9a5yhxUw.png"></figure><p name="e69d" id="e69d" class="graf graf--p graf-after--figure">(b) <strong class="markup--strong markup--p-strong">E-UNet </strong>bottleneck modulus. The convolution is either a regular, dilated, or full convolution with 3×3 filters or a 5×5 convolution decomposed into two asymmetric ones.</p><p name="85f6" id="85f6" class="graf graf--p graf-after--p">As seen below, the architecture of the <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">contraction section </em></strong>is divided into several stages as highlighted by the horizontal lines in the table.</p><figure name="4ec8" id="4ec8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_11ROvh4wAbgKmIBgIlQ1Q.png" data-width="1072" data-height="1592" src="https://cdn-images-1.medium.com/max/800/1*_11ROvh4wAbgKmIBgIlQ1Q.png"><figcaption class="imageCaption">Architecture of the <em class="markup--em markup--figure-em">contraction section</em></figcaption></figure><h4 name="b1f4" id="b1f4" class="graf graf--h4 graf-after--figure">2.2. <strong class="markup--strong markup--h4-strong"><em class="markup--em markup--h4-em">The expansion section:</em></strong></h4><p name="1039" id="1039" class="graf graf--p graf-after--h4">This section concerns about the location of object of interest in images and consists of combination of convolutions (Conv2D) and De-convolution (Conv2DTranspose) or up-sampling (UpSampling2D) layers.</p><figure name="2cc3" id="2cc3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PXACI9GQq_5GkEW2yQnVxw.png" data-width="1062" data-height="1242" src="https://cdn-images-1.medium.com/max/800/1*PXACI9GQq_5GkEW2yQnVxw.png"><figcaption class="imageCaption">other semantic segmentation models,Architecture of the <em class="markup--em markup--figure-em">expansion section</em></figcaption></figure><h4 name="63e9" id="63e9" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong"><em class="markup--em markup--h4-em">2.3. The skip connection:</em></strong></h4><p name="5c81" id="5c81" class="graf graf--p graf-after--h4">As seen below, the feature maps of the contraction section are merged with expansion features to improve localization and presentations.</p><figure name="1a9f" id="1a9f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*EyXHAkKVAGhrq0IflKY3oA.png" data-width="1936" data-height="1376" src="https://cdn-images-1.medium.com/max/800/1*EyXHAkKVAGhrq0IflKY3oA.png"><figcaption class="imageCaption">Skip connections</figcaption></figure><h3 name="3d04" id="3d04" class="graf graf--h3 graf-after--figure">3. Model Parameters:</h3><p name="060b" id="060b" class="graf graf--p graf-after--h3">By making a comparison between the number of parameters of <strong class="markup--strong markup--p-strong">the E-UNet</strong> and <strong class="markup--strong markup--p-strong">other semantic segmentation models,</strong> we see there is a big difference which results in <strong class="markup--strong markup--p-strong">the fewer FLOPs (floating point operations per second).</strong></p><figure name="50c0" id="50c0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*XOtYMZttpN0M55Lm-LpJZQ.png" data-width="842" data-height="202" src="https://cdn-images-1.medium.com/max/800/1*XOtYMZttpN0M55Lm-LpJZQ.png"><figcaption class="imageCaption">The number of parameters of the E-UNet</figcaption></figure><figure name="ddd3" id="ddd3" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*E_9RpScU-5gmNudhd3PkTw.png" data-width="1358" data-height="1482" src="https://cdn-images-1.medium.com/max/800/1*E_9RpScU-5gmNudhd3PkTw.png"><figcaption class="imageCaption">The number of parameters of the semantic segmentation models</figcaption></figure><h3 name="87d8" id="87d8" class="graf graf--h3 graf-after--figure">4. Train and test Model</h3><p name="b4a5" id="b4a5" class="graf graf--p graf-after--h3">There are many loss functions to use for semantic segmentation problems but the current approach decided for the most useful which is <strong class="markup--strong markup--p-strong">Binary Cross Entropy</strong>.</p><p name="03bb" id="03bb" class="graf graf--p graf-after--p">After selecting the loss function training, validation, and test datasets are needed to train and evaluate the <strong class="markup--strong markup--p-strong">E-UNet. The training, validation, and test datasets explained in the fourth section of the </strong><a href="https://github.com/A2Amir/Pix2Pix-for-Semantic-Segmentation-of-Satellite-Images" data-href="https://github.com/A2Amir/Pix2Pix-for-Semantic-Segmentation-of-Satellite-Images" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Pix2Pix-for-Semantic-Segmentation-of-Satellite-Images</strong></a><strong class="markup--strong markup--p-strong"> repository were used.</strong></p><p name="43c2" id="43c2" class="graf graf--p graf-after--p"><a href="https://github.com/A2Amir/Pix2Pix-for-Semantic-Segmentation-of-Satellite-Images/tree/master/Dataset/Klein_Dataset2" data-href="https://github.com/A2Amir/Pix2Pix-for-Semantic-Segmentation-of-Satellite-Images/tree/master/Dataset/Klein_Dataset2" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">These datasets</a> consist of two categories including satellite images and corresponding ground truths to present where buildings in the satellite images are.</p><figure name="5462" id="5462" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PBaE11cSGobgXMAOieMdMA.png" data-width="1108" data-height="468" src="https://cdn-images-1.medium.com/max/800/1*PBaE11cSGobgXMAOieMdMA.png"><figcaption class="imageCaption"></figcaption></figure><p name="1114" id="1114" class="graf graf--p graf-after--figure">After training the <strong class="markup--strong markup--p-strong">E-UNet</strong>, <strong class="markup--strong markup--p-strong">a validation loss of 0.12041</strong>(without any fine tuning) was achieved which is pretty efficient. Then predictions about the unseen data (test dataset) were made in order to detect the location of building in satellite images (see images below).</p><figure name="e4a4" id="e4a4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*8VkEXmmGE0iEMhNGiNNo1w.png" data-width="2416" data-height="802" src="https://cdn-images-1.medium.com/max/800/1*8VkEXmmGE0iEMhNGiNNo1w.png"></figure><figure name="d4d3" id="d4d3" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*uxzU8MGdwDWo1B4YesEQ6w.png" data-width="2410" data-height="784" src="https://cdn-images-1.medium.com/max/800/1*uxzU8MGdwDWo1B4YesEQ6w.png"></figure><figure name="4bdb" id="4bdb" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*9YWOqZ95hV1haQqwEwiKYA.png" data-width="2406" data-height="780" src="https://cdn-images-1.medium.com/max/800/1*9YWOqZ95hV1haQqwEwiKYA.png"></figure><p name="4a51" id="4a51" class="graf graf--p graf-after--figure">As shown, the E-UNet is a powerful architecture for real-time semantic segmentation, especially if one wants to run it in low-resource embedded systems.</p><blockquote name="69c0" id="69c0" class="graf graf--blockquote graf-after--p"><em class="markup--em markup--blockquote-em">you can find all files (trained weights, model, ..)related to the </em><a href="https://github.com/A2Amir/A-new-efficient-lightweight-semantic-segmentation-Network" data-href="https://github.com/A2Amir/A-new-efficient-lightweight-semantic-segmentation-Network" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--blockquote-strong"><em class="markup--em markup--blockquote-em">projects on my github</em></strong></a><em class="markup--em markup--blockquote-em"> .</em></blockquote><h3 name="4467" id="4467" class="graf graf--h3 graf-after--blockquote graf--trailing">Amir Ziaee</h3></div></div></section><section name="a3b8" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><div name="c7c3" id="c7c3" class="graf graf--mixtapeEmbed graf--leading"></section>
</section>
</article>
</body>
</html>
