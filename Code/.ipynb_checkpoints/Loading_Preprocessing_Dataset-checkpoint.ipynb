{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(Dataset_dir):\n",
    "    \n",
    "    SB_dir = os.path.join(Dataset_dir, 'Croped_SB/') #Satellite Bilder path\n",
    "    GT_dir = os.path.join(Dataset_dir, 'Croped_GT/')# Ground Truths path\n",
    "\n",
    "\n",
    "    \n",
    "    SB_listnames=glob.glob(SB_dir+\"*.png\")#Satellite Bilder filenames\n",
    "    GT_listnames=glob.glob(GT_dir+\"*.png\")# Ground Truths filenames\n",
    "    \n",
    "    GT_listnames.sort()\n",
    "    SB_listnames.sort()\n",
    "    \n",
    "    print(\"Satellite Directory:\",SB_dir)\n",
    "    print('Number of  ground truths:',len(SB_listnames))\n",
    "    print(\"\") \n",
    "    print(\"Ground Truths Directory:\",GT_dir)\n",
    "    print('Number der satellien images:',len(GT_listnames))\n",
    "\n",
    "    print(\"*********************************************\") \n",
    "\n",
    "    \n",
    "    return SB_dir, GT_dir, SB_listnames, GT_listnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_filter(image, sigma = 2):\n",
    "    img = np.copy(image)\n",
    "    blur = filters.gaussian(img, sigma=sigma)\n",
    "    return blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary(image, threshold, max_value = 1):\n",
    "    img = np.copy(image)\n",
    "    (t,masklayer) = cv2.threshold(img,threshold,max_value,cv2.THRESH_BINARY)\n",
    "    return masklayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_threshold_otsu(image):\n",
    "    t = filters.threshold_otsu(image)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(s_image_path, gt_image_path):\n",
    "    \n",
    "    s_image_path=str(s_image_path).split(\"'\")[1]\n",
    "    gt_image_path=str(gt_image_path).split(\"'\")[1]\n",
    "\n",
    "    #print(s_image_path, gt_image_path)   \n",
    "    s_image =cv2.imread(str(s_image_path))\n",
    "    s_image= cv2.cvtColor(s_image,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    #GT_path=str(GT_path).split(\"'\")[1]\n",
    "    gt_image =cv2.imread(str(gt_image_path))\n",
    "    gt_image= cv2.cvtColor(gt_image,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "    if np.random.random() > 0.5:\n",
    "     # random mirroring\n",
    "        s_image = cv2.flip( s_image, -1);\n",
    "        gt_image = cv2.flip( gt_image, -1);\n",
    "\n",
    "\n",
    "        \n",
    "    gt_image= cv2.cvtColor(gt_image,cv2.COLOR_RGB2GRAY)\n",
    "     \n",
    "        \n",
    "    guass_img = gaussian_filter(gt_image, sigma =2)\n",
    "    threshold =  find_threshold_otsu(guass_img)\n",
    "    gt_image = binary(guass_img, threshold, max_value = 1)\n",
    "    \n",
    "    s_image = s_image / 255.0\n",
    "\n",
    "    s_image = tf.cast(s_image, tf.float32)\n",
    "    gt_image = tf.cast(gt_image, tf.float32)\n",
    "    gt_image = tf.expand_dims( gt_image, 2)    \n",
    "\n",
    "\n",
    "    return s_image,gt_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(sb_list, gt_list, buffer_size =  2000, number_batche = 250 ):\n",
    "    \n",
    "    sb_dataset = tf.data.Dataset.from_tensor_slices(sb_list)\n",
    "    gt_dataset = tf.data.Dataset.from_tensor_slices(gt_list)\n",
    "    dataset = tf.data.Dataset.zip((sb_dataset, gt_dataset))\n",
    "    dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.map(lambda x,y: tf.py_function(load_image, [x, y], [tf.float32,tf.float32]))\n",
    "    dataset = dataset.batch(number_batche)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import argparse\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup\n",
    "from skimage import io as skio\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "\n",
    "def html_url_parser(url, save_dir, show=False, wait=False):\n",
    "    \"\"\"\n",
    "    HTML parser to download images from URL.\n",
    "    Params:\\n\n",
    "    `url` - Image url\\n\n",
    "    `save_dir` - Directory to save extracted images\\n\n",
    "    `show` - Show downloaded image\\n\n",
    "    `wait` - Press key to continue executing\n",
    "    \"\"\"\n",
    "\n",
    "    website = urlopen(url)\n",
    "    html = website.read()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html5lib\")\n",
    "\n",
    "    for image_id, link in enumerate(soup.find_all('a', href=True)):\n",
    "        if(image_id == 0):\n",
    "            continue\n",
    "        \n",
    " \n",
    "        img_url = link['href']\n",
    "\n",
    "        try:\n",
    "            if os.path.isfile(save_dir + \"img-%d.png\" % image_id) == False:\n",
    "                print(\"[INFO] Downloading image from URL:\", link['href'])\n",
    "                image = Image.open(urlopen(img_url))\n",
    "                image.save(save_dir + \"img-%d.png\" % image_id, \"PNG\")\n",
    "                if(show):\n",
    "                    image.show()\n",
    "            else:\n",
    "                print('skipped')\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"[EXCEPTION] Pressed 'Ctrl+C'\")\n",
    "            break\n",
    "        except Exception as image_exception:\n",
    "            print(\"[EXCEPTION]\", image_exception)\n",
    "            continue\n",
    "\n",
    "        if(wait):\n",
    "            key = input(\"[INFO] Press any key to continue ('q' to exit)... \")\n",
    "            if(key.lower() == 'q'):\n",
    "                break\n",
    "\n",
    "# ///////////////////////////////////////////////////\n",
    "#                   Main method\n",
    "# ///////////////////////////////////////////////////\n",
    "if __name__ == \"__main__\":\n",
    "    URL_TRAIN_IMG = \"https://www.cs.toronto.edu/~vmnih/data/mass_roads/train/sat/index.html\"\n",
    "    URL_TRAIN_GT = \"https://www.cs.toronto.edu/~vmnih/data/mass_roads/train/map/index.html\"\n",
    "\n",
    "    URL_TEST_IMG = \"https://www.cs.toronto.edu/~vmnih/data/mass_roads/valid/sat/index.html\"\n",
    "    URL_TEST_GT = \"https://www.cs.toronto.edu/~vmnih/data/mass_roads/valid/map/index.html\"\n",
    "\n",
    "    \n",
    "    html_url_parser(url=URL_TRAIN_IMG, save_dir=\"./road_segmentation/training/input/\")\n",
    "    html_url_parser(url=URL_TRAIN_GT, save_dir=\"./road_segmentation/training/output/\")\n",
    "\n",
    "    html_url_parser(url=URL_TEST_IMG, save_dir=\"./road_segmentation/testing/input/\")\n",
    "    html_url_parser(url=URL_TEST_GT, save_dir=\"./road_segmentation/testing/output/\")\n",
    "\n",
    "    print(\"[INFO] All done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
